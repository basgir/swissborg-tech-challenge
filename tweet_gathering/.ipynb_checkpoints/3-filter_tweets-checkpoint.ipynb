{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Classifying Tweets as spam or ham\n",
    "## Comments\n",
    "In this section we are filtering our tweets. To do so we are training a model that performs a classifcation.\n",
    "Details of the model are mentionned below.\n",
    "\n",
    "### Data\n",
    "In order for the classifier to be efficient, we needed good labeled data. First we looked at the free sources data quality. For example : SPAM base on kaggle, SMS spam on UCI machine learning, etc... However, the results weren't good enough. In general, the quality of the free available data wasn't good and specific enough and thus, was leading to poor classification results.\n",
    "To improve our spam vs ham classifier we needed a solution that would be efficient enough.\n",
    "We found the solution to be hand labeling the tweets. It would be specific enough to lead to good classification results. But, this was a tedious process. Furthermore it we needed a sufficiently large enough database of hand labeled data. We found a remedy in designing an algorithm that would label this data for us. But how ? Since Twitter was already doing a quite good job of eliminating spam tweets, so there was no need for an extremly accurate algorithm. \n",
    "\n",
    "### Labeled data\n",
    "To do so, we first created a list of spam words. That would help in checking whether the tweet is a spam. Then we gathered tweets from various sectors. With that data, we developped an agorithm that goes through each tweets and sees if one spam words is in the tweets if yes, the tweet is classified as spam.\n",
    "\n",
    "Overall, our algorithm isn't perfect, a more fined-grained list of spam words would lead to more efficiency but it does perfoms well enough for the purpose of this project.\n",
    "\n",
    "### Classification\n",
    "Further below in this section we show how we train our Spam classifier on the labeled data we got from our previous algorithm. \n",
    "Then how we classify our freshly gathered clean tweets.\n",
    "\n",
    "Finally, we'll save our ham tweets in this file:\n",
    "- **twitter_data_clean_ham.csv** : contained in */7-Data/2-CleanTweets/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Tweets Classification\n",
    "\n",
    "In Natural Lanugage processing (NLP) in order to classify text, we need a way to transform text into a form that the machine can understand and analyze it. We call it feature extractor. Then we need a way to tell the machine which are spam and which are ham, and the machine will find the way to separate them into two groups. This step is called classification.\n",
    "\n",
    "In our algorithm, we will use two features extractor, one will be for vectorizing the text into count of each words. These counts will help the machine understand the recurrence of certain words in a spam for example. Then we will apply a tfidf transformer which is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus, in our case the tweet. Example (\"Viagra\") will have more importance than (\"kicking\").\n",
    "\n",
    "We now perform the training of our classifier on training dataset of our good labeled data then we test it on the fresh tweets we gathered.\n",
    "For efficency and readibility purpose we use scikit-learn pipeline to train our classifier. More information available <a src=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\"> **here**</a>. In our case it is used to apply subsequent models on a specific data. Since we are applying a *Count Vectorizer*, then a *tfidf Transformer* and finally a *Multinomial Naïve Bayes*\n",
    "\n",
    "However, we also programmed a more detailed walkthrough that helps the user to understand the steps of our subsequent model. \n",
    "\n",
    "Which is available in the file : **training_classifier_details.ipynb** in folder */3-Spam-Filter/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "**Supervised classification depiction, retrieved in (Bird and Klein)**\n",
    "<img src=\"../6-Graphs/1-Sentiment/nlp-howto.png\" />\n",
    "\n",
    "source : Bird, S., Klein, E. & Loper, E. (2009). Natural Language Processing with Python: *Analyzing Text with the Natural Language Toolkit.* O'Reilly Media\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As depicted on the figure above: we are getting the data from the good labeled dataset (input and label) feeding it to the count vectorizer and the tfidf transformer (our features extractors). Then once the features have been extracted, we are feeding the results into our multinomial Naive Bayes classifier. Which will learn and tell us if it is a spam or a ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions of the classifier\n",
    "#### Why using pickle file extension ?\n",
    "- First, because computing a model takes time and computational power. So we'd rather save it in order not to have to compute it a second time. \n",
    "- Second, because such models can be huge depending on how they have been trained and the picke file system handles well huge files.\n",
    "The user is not obliged to re-run the following code and can directly go to the **Using the model to classify part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "    \"\"\"Classifier Trainer\"\"\"\n",
    "    \n",
    "    # We load training data\n",
    "    data = pd.read_csv(\"../7-Data/4-Corpus/twitter_spam_trainer.csv\", encoding=\"utf-8\", header=0)\n",
    "    \n",
    "    # We format the columns\n",
    "    data.columns = ['Text','SpamOrHam']\n",
    "    \n",
    "    # If some are missing we just don't count them as spam\n",
    "    data = data.fillna(\"spam\")\n",
    "    \n",
    "    # We split the data into training and testing data\n",
    "    msk_data = np.random.rand(len(data)) < 0.8\n",
    "    train = data[msk_data]\n",
    "    test = data[~msk_data]\n",
    "    \n",
    "    # We define our classifier pipeline\n",
    "    print('Training classifier...')\n",
    "    cl = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB())])\n",
    "    \n",
    "    # We fit our training data into the scikit pipeline : countvectorizer => tfidf transform => multinomialNB\n",
    "    cl = cl.fit(train.Text, train.SpamOrHam)    \n",
    "    print('Classifier trained')\n",
    "    \n",
    "    print('Computing score...')\n",
    "    \n",
    "    # We print the score\n",
    "    predicted = cl.predict(test.Text)  \n",
    "    print('Accuracy: {}'.format(np.mean(predicted == test['SpamOrHam'])))\n",
    "    \n",
    "    print('Saving model...')\n",
    "  \n",
    "    # saving model\n",
    "    joblib.dump(cl, '../7-Data/3-SpamModels/twitter_sentiment_model_spam.pkl')\n",
    "    print('Model saved.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and testing it\n",
    "We can see that our model has around 89% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Classifier trained\n",
      "Computing score...\n",
      "Accuracy: 0.8972364303577937\n",
      "Saving model...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "train_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "    # to load back in\n",
    "    cl = joblib.load('../7-Data/3-SpamModels/twitter_sentiment_model_spam.pkl')\n",
    "    print(\"Done.\")\n",
    "    return cl\n",
    "\n",
    "def predict_dataset(tweets, cl):\n",
    "    \"\"\"Predict if the tweets are spam or not\n",
    "    \n",
    "    Arguments:\n",
    "        tweets {string} -- tweets\n",
    "        cl {model pipeline} -- Model that is used to predict spam or ham\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame -- final predicted results\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict dataset\n",
    "    print('Pedict dataset...')\n",
    "\n",
    "    # We predict\n",
    "    predicted = cl.predict(tweets['Processed Text'].values.astype('U'))\n",
    "    \n",
    "    # We compound Text and predicted into a single dataframe\n",
    "    tweets['Predicted'] = predicted\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Return the final dataframe\n",
    "    return tweets\n",
    "\n",
    "def create_cloud(tweets):\n",
    "    \"\"\"Genereate a cloud of words that shows what are the most recurring words.\n",
    "    \n",
    "    Arguments:\n",
    "        tweets {[string]} -- list of tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "        max_words=200,\n",
    "        stopwords=stopwords, \n",
    "        width=800, \n",
    "        height=400)\n",
    "    wc.generate(tweets)\n",
    "    wc.to_file('../6-Graphs/0-Tweets/wordcloud.png')\n",
    "\n",
    "def load_tweets():\n",
    "    print(\"Loading tweets...\")\n",
    "    df = pd.read_csv('../7-Data/2-CleanTweets/twitter_data_clean.csv',  encoding='utf-8', index_col=0)\n",
    "    print(\"Done.\")\n",
    "    return df  \n",
    "\n",
    "def save_dataset(df):\n",
    "    print(\"Saving dataset...\")\n",
    "    df.to_csv('../7-Data/2-CleanTweets/twitter_data_clean_ham.csv')\n",
    "    print(\"Done. \\nSaved as : twitter_data_clean_ham.csv \")\n",
    "\n",
    "def clean_tweets(df):\n",
    "    # We eliminate nan fields\n",
    "    df = df[pd.notnull(df['Processed Text'])]\n",
    "\n",
    "    # We eliminate doubles\n",
    "    df = df.drop_duplicates('Message ID')\n",
    "    df = df.drop_duplicates('Processed Text')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main \n",
    "we run classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n",
      "Loading tweets...\n",
      "Done.\n",
      "Pedict dataset...\n",
      "Done.\n",
      "--------------------------------------------------\n",
      "Spam number :5538\t|\tHam number :13675\n",
      "--------------------------------------------------\n",
      "Generating wordcloud...\n",
      "Done. \n",
      "Saved as wordcloud.png\n",
      "Saving dataset...\n",
      "Done. \n",
      "Saved as : twitter_data_clean_ham.csv \n"
     ]
    }
   ],
   "source": [
    "# main program\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # train_classifier()\n",
    "\n",
    "    # We load the model\n",
    "    model_cl = load_model()\n",
    "\n",
    "    # We load the tweets\n",
    "    df_tweets = load_tweets()\n",
    "\n",
    "    # We gather results.\n",
    "    results = predict_dataset(df_tweets, model_cl)\n",
    "\n",
    "    results = clean_tweets(results)\n",
    "\n",
    "    # Spam Or Ham\n",
    "    spam = results['Predicted']==\"spam\"\n",
    "    ham = results['Predicted']==\"ham\"\n",
    "\n",
    "    # We print out the results.\n",
    "    print(\"-\"*50)\n",
    "    print('Spam number :{0}\\t|\\tHam number :{1}'.format(len(results[spam]['Predicted']),len(results[ham]['Predicted'])))  \n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # We generate a wordcloud of the most recurrent words.\n",
    "    print(\"Generating wordcloud...\")\n",
    "    create_cloud(results[ham]['Processed Text'].to_csv(encoding='utf-8', sep=' ', index=False, header=False))\n",
    "    print(\"Done. \\nSaved as {0}\".format('wordcloud.png'))\n",
    "    \n",
    "    # We save our dataset as twitter_data_clean_ham.csv\n",
    "    save_dataset(results[ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Processed Text</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Bean Technologies Corporation</td>\n",
       "      <td>James Sikes</td>\n",
       "      <td>It Seems John Bean Technologies Corporation (N...</td>\n",
       "      <td>9.975034e+17</td>\n",
       "      <td>2018-05-18 15:45:13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It Seems John Bean Technologies Corporation (N...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Argan, Inc.</td>\n",
       "      <td>TheOlympiaReport</td>\n",
       "      <td>Argan, Inc. $AGX Receives Average Rating of “S...</td>\n",
       "      <td>9.985680e+17</td>\n",
       "      <td>2018-05-21 14:15:26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Argan, . $AGX Receives Average Rating of “Stro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Despegar.com, Corp.</td>\n",
       "      <td>The Lincolnian</td>\n",
       "      <td>https://t.co/7swFWIccbw, Corp. $DESP Forecaste...</td>\n",
       "      <td>9.967057e+17</td>\n",
       "      <td>2018-05-16 10:55:23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Corp. $DESP Forecasted to Post Q2 2018 Earning...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arrow Electronics, Inc.</td>\n",
       "      <td>thingsofiot</td>\n",
       "      <td>Arrow Electronics, Inc. is hiring a Business D...</td>\n",
       "      <td>9.976463e+17</td>\n",
       "      <td>2018-05-19 01:13:06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Arrow Electronics, . is hiring a Business Deve...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Civeo Corporation</td>\n",
       "      <td>Jobs in Edmonton AB</td>\n",
       "      <td>Payroll Administrator (23014): Civeo Corporati...</td>\n",
       "      <td>9.942664e+17</td>\n",
       "      <td>2018-05-09 17:22:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Payroll Administrator (23014): Civeo Corporati...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Company Name          Author Name  \\\n",
       "0  John Bean Technologies Corporation          James Sikes   \n",
       "7                         Argan, Inc.     TheOlympiaReport   \n",
       "3                 Despegar.com, Corp.       The Lincolnian   \n",
       "0             Arrow Electronics, Inc.          thingsofiot   \n",
       "4                   Civeo Corporation  Jobs in Edmonton AB   \n",
       "\n",
       "                                                Text    Message ID  \\\n",
       "0  It Seems John Bean Technologies Corporation (N...  9.975034e+17   \n",
       "7  Argan, Inc. $AGX Receives Average Rating of “S...  9.985680e+17   \n",
       "3  https://t.co/7swFWIccbw, Corp. $DESP Forecaste...  9.967057e+17   \n",
       "0  Arrow Electronics, Inc. is hiring a Business D...  9.976463e+17   \n",
       "4  Payroll Administrator (23014): Civeo Corporati...  9.942664e+17   \n",
       "\n",
       "          Published At  Retweet Count  Favorite Count  \\\n",
       "0  2018-05-18 15:45:13            0.0             0.0   \n",
       "7  2018-05-21 14:15:26            0.0             0.0   \n",
       "3  2018-05-16 10:55:23            0.0             0.0   \n",
       "0  2018-05-19 01:13:06            0.0             0.0   \n",
       "4  2018-05-09 17:22:28            0.0             0.0   \n",
       "\n",
       "                                      Processed Text Predicted  \n",
       "0  It Seems John Bean Technologies Corporation (N...       ham  \n",
       "7  Argan, . $AGX Receives Average Rating of “Stro...       ham  \n",
       "3  Corp. $DESP Forecasted to Post Q2 2018 Earning...       ham  \n",
       "0  Arrow Electronics, . is hiring a Business Deve...       ham  \n",
       "4  Payroll Administrator (23014): Civeo Corporati...       ham  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[ham].sample(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Processed Text</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ONEOK, Inc.</td>\n",
       "      <td>$Stocknewsalerts</td>\n",
       "      <td>$OKE ONEOK, Inc. email alerting service\\r\\nFro...</td>\n",
       "      <td>9.964853e+17</td>\n",
       "      <td>2018-05-15 20:19:49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$OKE ONEOK, . email alerting service From our ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Commerzbank AG</td>\n",
       "      <td>Mr Benn</td>\n",
       "      <td>@siemens_press @Siemens @IGMetall @JoeKaeser @...</td>\n",
       "      <td>9.971803e+17</td>\n",
       "      <td>2018-05-17 18:21:16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Das finde ich Klasse. Viele weiteren...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Crane Company</td>\n",
       "      <td>Lupprians NL</td>\n",
       "      <td>Lupprians HQ NL Installing new MRI's @ Erasmus...</td>\n",
       "      <td>9.974856e+17</td>\n",
       "      <td>2018-05-18 14:34:42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lupprians HQ NL Installing new MRI's   Erasmus...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Scana Corporation</td>\n",
       "      <td>Julia Duncan</td>\n",
       "      <td>Cayce, S.C., le 26 Octobre, 2017 .. récupérer ...</td>\n",
       "      <td>9.946847e+17</td>\n",
       "      <td>2018-05-10 21:04:56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cayce, S.C., le 26 Octobre, 2017 .. récupérer ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xylem Inc.</td>\n",
       "      <td>Mobile Deals</td>\n",
       "      <td>https://t.co/oFAxxXZhpf *Free Lyft rides for a...</td>\n",
       "      <td>9.977052e+17</td>\n",
       "      <td>2018-05-19 05:07:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>*Free Lyft rides for all new users &amp;lt;&amp;lt; Us...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Company Name       Author Name  \\\n",
       "6         ONEOK, Inc.  $Stocknewsalerts   \n",
       "7      Commerzbank AG           Mr Benn   \n",
       "11      Crane Company      Lupprians NL   \n",
       "9   Scana Corporation      Julia Duncan   \n",
       "0          Xylem Inc.      Mobile Deals   \n",
       "\n",
       "                                                 Text    Message ID  \\\n",
       "6   $OKE ONEOK, Inc. email alerting service\\r\\nFro...  9.964853e+17   \n",
       "7   @siemens_press @Siemens @IGMetall @JoeKaeser @...  9.971803e+17   \n",
       "11  Lupprians HQ NL Installing new MRI's @ Erasmus...  9.974856e+17   \n",
       "9   Cayce, S.C., le 26 Octobre, 2017 .. récupérer ...  9.946847e+17   \n",
       "0   https://t.co/oFAxxXZhpf *Free Lyft rides for a...  9.977052e+17   \n",
       "\n",
       "           Published At  Retweet Count  Favorite Count  \\\n",
       "6   2018-05-15 20:19:49            0.0             0.0   \n",
       "7   2018-05-17 18:21:16            0.0             4.0   \n",
       "11  2018-05-18 14:34:42            1.0             1.0   \n",
       "9   2018-05-10 21:04:56            0.0             0.0   \n",
       "0   2018-05-19 05:07:04            0.0             0.0   \n",
       "\n",
       "                                       Processed Text Predicted  \n",
       "6   $OKE ONEOK, . email alerting service From our ...      spam  \n",
       "7             Das finde ich Klasse. Viele weiteren...      spam  \n",
       "11  Lupprians HQ NL Installing new MRI's   Erasmus...      spam  \n",
       "9   Cayce, S.C., le 26 Octobre, 2017 .. récupérer ...      spam  \n",
       "0   *Free Lyft rides for all new users &lt;&lt; Us...      spam  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[spam].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Conclusion\n",
    "To conclude, we sadly, can see that our algorithm isn't perfect. We realized the importance of good data. Machine learning algorithms only performs well when there is high quality in the data. Furthermore, we looked for freely available data and did not take any costly solution into account. To improve the accuracy of our algorithm one solution would be to better understand how a spam is structured and find characteristics that differentiate it to a ham. That could be done by adding additional requirements that a tweet should pass in order to be classified as ham. Such as for example **weird characters**, **smileys**, **SMS words**, **better filter for languages**, **etc...**\n",
    "\n",
    "Our main concern would reside in the fact that we are more **interested** in elminating the **false negative** than we are in eliminating **false positive**. As example, that would matter more that a **spam is classified as ham** than the inverse. Our **false negative** ham would lead to a **bias** in our sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the user may go to Part - 4 Sentiment Analysis\n",
    "File *sentiment_analysis.ipynb* in folder **4-Sentiment**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning]",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
