{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Tweets cleaning\n",
    "## Comments\n",
    "In this section we are cleaning the tweets, formating it to utf8, removing all `#` and `@` aswell as the links and other useless words or acronyms.\n",
    "Then we finally save the procesed text of the tweets into a dataframe that is in turn saved as a csv file:\n",
    "- **twitter_data_clean.csv** : contained in */7-Data/2-CleanTweets/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re, string\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Strip links and entities\n",
    "def strip_tweet(text):\n",
    "\n",
    "    # find all links\n",
    "    links = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    # for each link found\n",
    "    for link in links:\n",
    "\n",
    "        # replace the link with a space\n",
    "        text = text.replace(link, ' ')\n",
    "\n",
    "    # entity prefixes\n",
    "    entity_prefixes = ['@','#']\n",
    "    text1 = text\n",
    "    text = ' '.join([line.strip() for line in text1.strip().splitlines()])\n",
    "\n",
    "    # for each word in the tweet\n",
    "    for idx, word in enumerate(text.split()):\n",
    "\n",
    "        # for each letter in the word\n",
    "        for letter in word:\n",
    "\n",
    "            # if the letter is a @ or #\n",
    "            if letter in entity_prefixes:\n",
    "\n",
    "                # replace the word with a space\n",
    "                text = text.replace(word, ' ')\n",
    "    \n",
    "    # Remove various unimportant texts\n",
    "    \n",
    "    # We delete the RT that mean Retweeted\n",
    "    text = text.replace('RT','')\n",
    "    \n",
    "    # We delete the text Form\n",
    "    text = text.replace('Form','')\n",
    "    \n",
    "    # We delete Inc since we know we are talking about corporations\n",
    "    text = text.replace('Inc','')\n",
    "    \n",
    "    # We delete App since it is not considered as important\n",
    "    text = text.replace('App','')\n",
    "    \n",
    "    # We delete Alerts since it is not considered as important\n",
    "    text = text.replace('Alerts','')\n",
    "    \n",
    "    # return the processed text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We load our companies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading twitter_data.csv\n",
      "../7-Data/1-RawTweets/twitter_data.csv\n",
      "Failed to load twitter_data.csv\n",
      "Loading twitter_data_1527428653.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527428653.csv\n",
      "Loading twitter_data_1527456526.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527456526.csv\n",
      "Loading twitter_data_1527693686.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527693686.csv\n",
      "Loading twitter_data_1527693856.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527693856.csv\n",
      "Loading twitter_data_1527856125.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527856125.csv\n",
      "Loading twitter_data_1527873917.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527873917.csv\n",
      "Loading twitter_data_1527949217.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527949217.csv\n",
      "Loading twitter_data_1527952342.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1527952342.csv\n",
      "Loading twitter_data_1528037772.csv\n",
      "../7-Data/1-RawTweets/twitter_data_1528037772.csv\n",
      "             Company Name            Author Name  \\\n",
      "0  3D Systems Corporation  takai backpack N k.L.   \n",
      "1  3D Systems Corporation  takai backpack N k.L.   \n",
      "2  3D Systems Corporation  takai backpack N k.L.   \n",
      "3  3D Systems Corporation            3dprintfeed   \n",
      "4  3D Systems Corporation            3dprintfeed   \n",
      "\n",
      "                                                Text    Message ID  \\\n",
      "0  RT @louisiananews1: 3D and 4D Technology Marke...  1.000427e+18   \n",
      "1  RT @GhaziabadOnline: 3D and 4D Technology Mark...  1.000427e+18   \n",
      "2  RT @ABNewswire: 3D and 4D Technology Market By...  1.000427e+18   \n",
      "3  New York State Common Retirement Fund Reduced ...  1.000412e+18   \n",
      "4  3D Systems Corporation (DDD) Shares Obtained b...  1.000334e+18   \n",
      "\n",
      "          Published At  Retweet Count  Favorite Count  \\\n",
      "0  2018-05-26 17:21:57            1.0             0.0   \n",
      "1  2018-05-26 17:21:54            1.0             0.0   \n",
      "2  2018-05-26 17:21:51            1.0             0.0   \n",
      "3  2018-05-26 16:21:42            0.0             0.0   \n",
      "4  2018-05-26 11:11:23            0.0             0.0   \n",
      "\n",
      "                                      Processed Text  \n",
      "0     3D and 4D Technology Market By Product Type...  \n",
      "1     3D and 4D Technology Market By Product Type...  \n",
      "2     3D and 4D Technology Market By Product Type...  \n",
      "3  New York State Common Retirement Fund Reduced ...  \n",
      "4  3D Systems Corporation (DDD) Shares Obtained b...  \n",
      "Files Loaded.\n",
      "File Saved.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load companies\n",
    "companies = pd.read_csv('./data/crypto.csv',index_col=0, encoding=\"utf-8\")\n",
    "\n",
    "companies.index = companies['Name']\n",
    "\n",
    "# discover tweet files\n",
    "datapath='../data/'\n",
    "files = [f for f in listdir(datapath) if isfile(join(datapath, f))]\n",
    "\n",
    "# load tweets\n",
    "dfs = []\n",
    "\n",
    "# We go through all the tweets data files in our folder.\n",
    "for f in files:\n",
    "    \n",
    "    # Shows which file is currently processed\n",
    "    print('Loading {}'.format(f))\n",
    "    \n",
    "    # Full filepath\n",
    "    full_filepath = '{0}/{1}'.format(datapath, f)\n",
    "    \n",
    "    # We try to read the files\n",
    "    try:\n",
    "        df = pd.read_csv(full_filepath, encoding=\"utf-8\", index_col=0, engine='python')\n",
    "        \n",
    "        print(full_filepath)\n",
    "        \n",
    "        df['Processed Text'] = df['Text'].apply(strip_tweet)\n",
    "        \n",
    "        # We eliminate duplicates tweet\n",
    "        df = df.drop_duplicates(subset='Processed Text')\n",
    "        \n",
    "        dfs.append(df)\n",
    "    except:\n",
    "        print('Failed to load %s' % f)\n",
    "\n",
    "# We contatenate all tweets into one dataframe\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# We check.\n",
    "print(df.head())\n",
    "\n",
    "# Shows that it worked.\n",
    "print('Files Loaded.')\n",
    "\n",
    "# We save the clean all in one tweets dataframe into a csv\n",
    "df.to_csv('../7-Data/2-CleanTweets/twitter_data_clean.csv', header=True, encoding=\"utf-8\")\n",
    "\n",
    "print(\"File Saved.\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has been saved as *twitter_data_clean.csv* in **/7-Data/2-CleanTweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the user may go to Part 3 - Labelling data\n",
    "First the tweets labelling in order to generate our training data:\n",
    "\n",
    "- File *labelling_data.ipynb* in folder **3-Filter-Data**\n",
    "\n",
    "Secondly the training/testing/ prediction of the spam or ham classifier\n",
    " \n",
    "- File *filter_tweets.ipynb* in folder **3-Filter-Data**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
