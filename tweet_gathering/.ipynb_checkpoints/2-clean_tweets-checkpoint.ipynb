{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Tweets cleaning\n",
    "## Comments\n",
    "In this section we are cleaning the tweets, formating it to utf8, removing all `#` and `@` aswell as the links and other useless words or acronyms.\n",
    "Then we finally save the procesed text of the tweets into a dataframe that is in turn saved as a csv file:\n",
    "- **twitter_data_clean.csv** : contained in */7-Data/2-CleanTweets/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re, string\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Strip links and entities\n",
    "def strip_tweet(text):\n",
    "\n",
    "    # find all links\n",
    "    links = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    # for each link found\n",
    "    for link in links:\n",
    "\n",
    "        # replace the link with a space\n",
    "        text = text.replace(link, ' ')\n",
    "\n",
    "    # entity prefixes\n",
    "    entity_prefixes = ['@','#']\n",
    "    text1 = text\n",
    "    text = ' '.join([line.strip() for line in text1.strip().splitlines()])\n",
    "\n",
    "    # for each word in the tweet\n",
    "    for idx, word in enumerate(text.split()):\n",
    "\n",
    "        # for each letter in the word\n",
    "        for letter in word:\n",
    "\n",
    "            # if the letter is a @ or #\n",
    "            if letter in entity_prefixes:\n",
    "\n",
    "                # replace the word with a space\n",
    "                text = text.replace(word, ' ')\n",
    "    \n",
    "    # Remove various unimportant texts\n",
    "    \n",
    "    # We delete the RT that mean Retweeted\n",
    "    text = text.replace('RT','')\n",
    "    \n",
    "    # We delete the text Form\n",
    "    text = text.replace('Form','')\n",
    "    \n",
    "    # We delete Inc since we know we are talking about corporations\n",
    "    text = text.replace('Inc','')\n",
    "    \n",
    "    # We delete App since it is not considered as important\n",
    "    text = text.replace('App','')\n",
    "    \n",
    "    # We delete Alerts since it is not considered as important\n",
    "    text = text.replace('Alerts','')\n",
    "    \n",
    "    # return the processed text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We load our companies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading twitter_data_1567761122.csv\n",
      "./data/1-raw-tweets//twitter_data_1567761122.csv\n",
      "Loading twitter_data_1567761063.csv\n",
      "./data/1-raw-tweets//twitter_data_1567761063.csv\n",
      "Loading twitter_data_1567871525.csv\n",
      "./data/1-raw-tweets//twitter_data_1567871525.csv\n",
      "Loading twitter_data_1567761103.csv\n",
      "./data/1-raw-tweets//twitter_data_1567761103.csv\n",
      "Loading twitter_data_1567761153.csv\n",
      "./data/1-raw-tweets//twitter_data_1567761153.csv\n",
      "Loading twitter_data_1567760999.csv\n",
      "./data/1-raw-tweets//twitter_data_1567760999.csv\n",
      "Loading twitter_data_1567761359.csv\n",
      "./data/1-raw-tweets//twitter_data_1567761359.csv\n",
      "Loading twitter_data_1567760411.csv\n",
      "./data/1-raw-tweets//twitter_data_1567760411.csv\n",
      "Loading twitter_data_1567760665.csv\n",
      "./data/1-raw-tweets//twitter_data_1567760665.csv\n",
      "Loading twitter_data_1567760683.csv\n",
      "./data/1-raw-tweets//twitter_data_1567760683.csv\n",
      "Loading twitter_data_1567761032.csv\n",
      "./data/1-raw-tweets//twitter_data_1567761032.csv\n",
      "     Author Name Company Name Crypto Favorite Count           Message ID  \\\n",
      "0   Arb Cruncher         DROP    NaN              0  1167950535074312192   \n",
      "2             SB         DROP    NaN              0  1167949452448940032   \n",
      "3   Arb Cruncher         DROP    NaN              0  1167927899283804161   \n",
      "5         Caulon         DROP    NaN              0  1167924633233838087   \n",
      "6  Startup Feeds         DROP    NaN              0  1167480724699508736   \n",
      "\n",
      "                                      Processed Text         Published At  \\\n",
      "0     Arbiswap BTC-CHSB Arbitrage Deal Alert!    ...  2019-09-01 00:01:21   \n",
      "2    Arbiswap BTC-CHSB Arbitrage Deal Alert!     via  2019-08-31 23:57:03   \n",
      "3        Arbiswap BTC-CHSB Arbitrage Deal Alert!      2019-08-31 22:31:24   \n",
      "5        Arbiswap BTC-CHSB Arbitrage Deal Alert!      2019-08-31 22:18:26   \n",
      "6     \"What you are what you do\" Thanks          ...  2019-08-30 16:54:30   \n",
      "\n",
      "  Retweet Count                                               Text  \n",
      "0             2  RT @amruthasuri: Arbiswap BTC-CHSB Arbitrage D...  \n",
      "2             2  Arbiswap BTC-CHSB Arbitrage Deal Alert! #Crypt...  \n",
      "3             3  RT @arbiswap: Arbiswap BTC-CHSB Arbitrage Deal...  \n",
      "5             3  Arbiswap BTC-CHSB Arbitrage Deal Alert! #arbit...  \n",
      "6             1  RT @jmestrella92: \"What you are what you do\"\\n...  \n",
      "Files Loaded.\n",
      "File Saved.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bgir/venv/data-science/lib/python3.6/site-packages/ipykernel_launcher.py:36: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load crypto\n",
    "crypto = pd.read_csv('./data/crypto.csv', encoding=\"utf-8\")\n",
    "\n",
    "# discover tweet files\n",
    "datapath='./data/1-raw-tweets/'\n",
    "files = [f for f in listdir(datapath) if isfile(join(datapath, f))]\n",
    "\n",
    "# load tweets\n",
    "dfs = []\n",
    "\n",
    "# We go through all the tweets data files in our folder.\n",
    "for f in files:\n",
    "    \n",
    "    # Shows which file is currently processed\n",
    "    print('Loading {}'.format(f))\n",
    "    \n",
    "    # Full filepath\n",
    "    full_filepath = '{0}/{1}'.format(datapath, f)\n",
    "    \n",
    "    # We try to read the files\n",
    "    try:\n",
    "        df = pd.read_csv(full_filepath, encoding=\"utf-8\", index_col=0, engine='python')\n",
    "        \n",
    "        print(full_filepath)\n",
    "        \n",
    "        df['Processed Text'] = df['Text'].apply(strip_tweet)\n",
    "        \n",
    "        # We eliminate duplicates tweet\n",
    "        df = df.drop_duplicates(subset='Processed Text')\n",
    "        \n",
    "        dfs.append(df)\n",
    "    except:\n",
    "        print('Failed to load %s' % f)\n",
    "\n",
    "# We contatenate all tweets into one dataframe\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# We check.\n",
    "print(df.head())\n",
    "\n",
    "# Shows that it worked.\n",
    "print('Files Loaded.')\n",
    "\n",
    "# We save the clean all in one tweets dataframe into a csv\n",
    "df.to_csv('./data/2-cleaned-tweets/cleaned_tweets.csv', header=True, encoding=\"utf-8\")\n",
    "\n",
    "print(\"File Saved.\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has been saved as *twitter_data_clean.csv* in **/7-Data/2-CleanTweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the user may go to Part 3 - Labelling data\n",
    "First the tweets labelling in order to generate our training data:\n",
    "\n",
    "- File *labelling_data.ipynb* in folder **3-Filter-Data**\n",
    "\n",
    "Secondly the training/testing/ prediction of the spam or ham classifier\n",
    " \n",
    "- File *filter_tweets.ipynb* in folder **3-Filter-Data**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
