{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Twitter ?\n",
    "We gather tweets since Twitter is known to be a financial information hub. \n",
    "We may emphazise on the fact that in general investor's behavior reflects on the market.\n",
    "\n",
    "More details may be found on these papers.\n",
    "> Bollen, Johan, Huina Mao, and Xiaojun Zeng. \"Twitter mood predicts the stock market.\" *Journal of computational science 2.1 (2011)*: 1-8.\n",
    "\n",
    "> Atkins, Adam, Mahesan Niranjan, and Enrico Gerding. \"Financial news predicts stock market volatility better than close price.\" *The Journal of Finance and Data Science 4.2 (2018)*: 120-137.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Fetching the tweets\n",
    "## Comments\n",
    "In this part we are gathering the tweets using the **tweepy library**. Since the API call rate of twitter is limited to 180 calls every 15 minutes, this process can take a while since we have around 3200 companies.\n",
    "We save the resulting process into a csv file of raw tweets namely in the **twitter_data.csv** in *7-Data/1-RawTweets*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We import the companies name and symbol\n",
    "We import our companies dataset that are publicly listed on SIX and NYSE.\n",
    "\n",
    "Data found on :\n",
    "- https://www.nasdaq.com/screening/companies-by-industry.aspx?exchange=NYSE for NYSE\n",
    "- https://www.six-swiss-exchange.com/shares/companies/issuer_list_en.html for SIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the csv(s) and select the relevant columns\n",
    "# SIX companies list\n",
    "df_crypto = pd.read_csv(\"./data/crypto.csv\")\n",
    "df_crypto = df_crypto[['symbol','symbol']]\n",
    "df_crypto.columns = ['Symbol','Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We authenticate on Twitter with our credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"z7gp32ZtNDlQOj4W92v6wJqm5\" \n",
    "consumer_secret = \"vbDEgRXlqALekExDz2wqhTiU6CwpBsLeEVNOF1al9hapqywsl6\" \n",
    "\n",
    "access_token = \"901719311143903233-V2t305dvpgFtMwonEIXoof8FOAKZxiH\"\n",
    "access_token_secret = \"z9knLifKkDRtJwgTQqxZxpA0Gy05HWVqsVj9K5M8fX0up\"\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We define our main functions\n",
    "1. **parse_tweet** : That we'll use to parse each tweets\n",
    "2. **format_response** : That we will use to transform in a company/tweet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is an error creating the api instance\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "# define parsing functions\n",
    "def parse_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Takes result object from tweepy and parses it\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize dict\n",
    "    parsed_tweet = {}\n",
    "\n",
    "    # extract relevant info\n",
    "    parsed_tweet['Author Name'] = tweet.author.name\n",
    "    parsed_tweet['Text'] = tweet.text\n",
    "    parsed_tweet['Message ID'] = tweet.id\n",
    "    parsed_tweet['Published At'] = tweet.created_at\n",
    "    parsed_tweet['Retweet Count'] = tweet.retweet_count\n",
    "    parsed_tweet['Favorite Count'] = tweet.favorite_count\n",
    "\n",
    "    return parsed_tweet\n",
    "\n",
    "def format_response(response, company):\n",
    "    \"\"\"\n",
    "    Takes list of result objects from tweepy and formats it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_tweets = pd.DataFrame([parse_tweet(tweet) for tweet in response], columns=[ 'Company Name', 'Author Name', 'Text', 'Message ID', 'Published At', 'Retweet Count', 'Favorite Count'])\n",
    "        parsed_tweets['Company Name'] = str(company)\n",
    "    except TypeError as e:\n",
    "        print(e)\n",
    "        parsed_tweets = pd.DataFrame(columns=[ 'Company Name', 'Author Name', 'Text', 'Message ID', 'Published At', 'Retweet Count', 'Favorite Count'])\n",
    "\n",
    "    return parsed_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main algorithm\n",
    "We now run the process of fetching tweets. Then we save it into a csv file namely: **twitter_data.csv** contained in the folder */7-Data/1-RawTweets*.\n",
    "\n",
    "This process is **quite time intensive**, so the user may totally **skip** it since tweets already have been fetched beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CHSB 0 of 7\n",
      "{'q': 'CHSB cryptocurrency'}\n",
      "Processing MCO 1 of 7\n",
      "{'q': 'MCO cryptocurrency'}\n",
      "Processing EDO 2 of 7\n",
      "{'q': 'EDO cryptocurrency'}\n",
      "Processing CRPT 3 of 7\n",
      "{'q': 'CRPT cryptocurrency'}\n",
      "Processing NEXO 4 of 7\n",
      "{'q': 'NEXO cryptocurrency'}\n",
      "Processing SXP 5 of 7\n",
      "{'q': 'SXP cryptocurrency'}\n",
      "Processing DROP 6 of 7\n",
      "{'q': 'DROP cryptocurrency'}\n"
     ]
    }
   ],
   "source": [
    "# main program\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # We create a timestamp\n",
    "    ts = int(time.time())\n",
    "    \n",
    "    # if twitter data exists\n",
    "    try:\n",
    "        data = pd.read_csv('./data/update.csv')\n",
    "    except:\n",
    "        data = False\n",
    "\n",
    "    # loop over companies\n",
    "    for idx,crypto in enumerate(df_crypto['Name']):\n",
    "\n",
    "        print('Processing {0} {1} of {2}'.format(crypto, str(idx), str(len(df_crypto['Name']))))\n",
    "\n",
    "        # define params dict\n",
    "        params={\n",
    "            'q' : crypto+\" cryptocurrency\"\n",
    "        }\n",
    "\n",
    "        # add max_id if prior data file exists\n",
    "        if data is not False:\n",
    "\n",
    "            # if this company exists in our dataset\n",
    "            if crypto in data['Company Name']:\n",
    "\n",
    "                # add the max_id param so we dont collect redundant tweets\n",
    "                params['since_id'] = data[data['Company Name']==crypto]['Message ID'].max()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            print(params)\n",
    "            # make the call to twitter\n",
    "            response = api.search(**params)\n",
    "\n",
    "        # handle error\n",
    "        except tweepy.error.TweepError as e:\n",
    "\n",
    "            print(e)\n",
    "            \n",
    "            # Will run up to the point where it reaches the Rate limit per 15 min.\n",
    "            response = api.search(**params)\n",
    "\n",
    "        # format response\n",
    "        formatted_response = format_response(response, crypto)\n",
    "        \n",
    "        # write out the result\n",
    "        if data is not False:\n",
    "            formatted_response.to_csv('./data/1-raw-tweets/twitter_data_{}.csv'.format(ts), mode='a', header=False, encoding='utf-8')\n",
    "        elif idx == 0:\n",
    "            formatted_response.to_csv('./data/1-raw-tweets/twitter_data_{}.csv'.format(ts), encoding='utf-8')\n",
    "        else:\n",
    "            formatted_response.to_csv('./data/1-raw-tweets/twitter_data_{}.csv'.format(ts), mode='a', header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the user may go to Part - 2 Tweets cleaning\n",
    "File *clean_tweets.ipynb* in folder **2-Clean-Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".data-science",
   "language": "python",
   "name": ".data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
