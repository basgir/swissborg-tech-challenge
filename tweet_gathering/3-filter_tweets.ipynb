{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Classifying Tweets as spam or ham\n",
    "## Comments\n",
    "In this section we are filtering our tweets. To do so we are training a model that performs a classifcation.\n",
    "Details of the model are mentionned below.\n",
    "\n",
    "### Data\n",
    "In order for the classifier to be efficient, we needed good labeled data. First we looked at the free sources data quality. For example : SPAM base on kaggle, SMS spam on UCI machine learning, etc... However, the results weren't good enough. In general, the quality of the free available data wasn't good and specific enough and thus, was leading to poor classification results.\n",
    "To improve our spam vs ham classifier we needed a solution that would be efficient enough.\n",
    "We found the solution to be hand labeling the tweets. It would be specific enough to lead to good classification results. But, this was a tedious process. Furthermore it we needed a sufficiently large enough database of hand labeled data. We found a remedy in designing an algorithm that would label this data for us. But how ? Since Twitter was already doing a quite good job of eliminating spam tweets, so there was no need for an extremly accurate algorithm. \n",
    "\n",
    "### Labeled data\n",
    "To do so, we first created a list of spam words. That would help in checking whether the tweet is a spam. Then we gathered tweets from various sectors. With that data, we developped an agorithm that goes through each tweets and sees if one spam words is in the tweets if yes, the tweet is classified as spam.\n",
    "\n",
    "Overall, our algorithm isn't perfect, a more fined-grained list of spam words would lead to more efficiency but it does perfoms well enough for the purpose of this project.\n",
    "\n",
    "### Classification\n",
    "Further below in this section we show how we train our Spam classifier on the labeled data we got from our previous algorithm. \n",
    "Then how we classify our freshly gathered clean tweets.\n",
    "\n",
    "Finally, we'll save our ham tweets in this file:\n",
    "- **twitter_data_clean_ham.csv** : contained in */7-Data/2-CleanTweets/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Tweets Classification\n",
    "\n",
    "In Natural Lanugage processing (NLP) in order to classify text, we need a way to transform text into a form that the machine can understand and analyze it. We call it feature extractor. Then we need a way to tell the machine which are spam and which are ham, and the machine will find the way to separate them into two groups. This step is called classification.\n",
    "\n",
    "In our algorithm, we will use two features extractor, one will be for vectorizing the text into count of each words. These counts will help the machine understand the recurrence of certain words in a spam for example. Then we will apply a tfidf transformer which is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus, in our case the tweet. Example (\"Viagra\") will have more importance than (\"kicking\").\n",
    "\n",
    "We now perform the training of our classifier on training dataset of our good labeled data then we test it on the fresh tweets we gathered.\n",
    "For efficency and readibility purpose we use scikit-learn pipeline to train our classifier. More information available <a src=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\"> **here**</a>. In our case it is used to apply subsequent models on a specific data. Since we are applying a *Count Vectorizer*, then a *tfidf Transformer* and finally a *Multinomial NaÃ¯ve Bayes*\n",
    "\n",
    "However, we also programmed a more detailed walkthrough that helps the user to understand the steps of our subsequent model. \n",
    "\n",
    "Which is available in the file : **training_classifier_details.ipynb** in folder */3-Spam-Filter/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As depicted on the figure above: we are getting the data from the good labeled dataset (input and label) feeding it to the count vectorizer and the tfidf transformer (our features extractors). Then once the features have been extracted, we are feeding the results into our multinomial Naive Bayes classifier. Which will learn and tell us if it is a spam or a ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions of the classifier\n",
    "#### Why using pickle file extension ?\n",
    "- First, because computing a model takes time and computational power. So we'd rather save it in order not to have to compute it a second time. \n",
    "- Second, because such models can be huge depending on how they have been trained and the picke file system handles well huge files.\n",
    "The user is not obliged to re-run the following code and can directly go to the **Using the model to classify part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "    \"\"\"Classifier Trainer\"\"\"\n",
    "    \n",
    "    # We load training data\n",
    "    data = pd.read_csv(\"./data/twitter_spam_trainer.csv\", encoding=\"utf-8\", header=0)\n",
    "    \n",
    "    # We format the columns\n",
    "    data.columns = ['Text','SpamOrHam']\n",
    "    \n",
    "    # If some are missing we just don't count them as spam\n",
    "    data = data.fillna(\"spam\")\n",
    "    \n",
    "    # We split the data into training and testing data\n",
    "    msk_data = np.random.rand(len(data)) < 0.8\n",
    "    train = data[msk_data]\n",
    "    test = data[~msk_data]\n",
    "    \n",
    "    # We define our classifier pipeline\n",
    "    print('Training classifier...')\n",
    "    cl = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB())])\n",
    "    \n",
    "    # We fit our training data into the scikit pipeline : countvectorizer => tfidf transform => multinomialNB\n",
    "    cl = cl.fit(train.Text, train.SpamOrHam)    \n",
    "    print('Classifier trained')\n",
    "    \n",
    "    print('Computing score...')\n",
    "    \n",
    "    # We print the score\n",
    "    predicted = cl.predict(test.Text)  \n",
    "    print('Accuracy: {}'.format(np.mean(predicted == test['SpamOrHam'])))\n",
    "    \n",
    "    print('Saving model...')\n",
    "  \n",
    "    # saving model\n",
    "    joblib.dump(cl, './data/twitter_sentiment_model_spam.pkl')\n",
    "    print('Model saved.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and testing it\n",
    "We can see that our model has around 89% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Classifier trained\n",
      "Computing score...\n",
      "Accuracy: 0.8962002783036016\n",
      "Saving model...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "train_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "    # to load back in\n",
    "    cl = joblib.load('./data/twitter_sentiment_model_spam.pkl')\n",
    "    print(\"Done.\")\n",
    "    return cl\n",
    "\n",
    "def predict_dataset(tweets, cl):\n",
    "    \"\"\"Predict if the tweets are spam or not\n",
    "    \n",
    "    Arguments:\n",
    "        tweets {string} -- tweets\n",
    "        cl {model pipeline} -- Model that is used to predict spam or ham\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame -- final predicted results\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict dataset\n",
    "    print('Pedict dataset...')\n",
    "\n",
    "    # We predict\n",
    "    predicted = cl.predict(tweets['Processed Text'].values.astype('U'))\n",
    "    \n",
    "    # We compound Text and predicted into a single dataframe\n",
    "    tweets['Predicted'] = predicted\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Return the final dataframe\n",
    "    return tweets\n",
    "\n",
    "def create_cloud(tweets):\n",
    "    \"\"\"Genereate a cloud of words that shows what are the most recurring words.\n",
    "    \n",
    "    Arguments:\n",
    "        tweets {[string]} -- list of tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "        max_words=200,\n",
    "        stopwords=stopwords, \n",
    "        width=800, \n",
    "        height=400)\n",
    "    wc.generate(tweets)\n",
    "    wc.to_file('./data/0-graphs/wordcloud.png')\n",
    "\n",
    "def load_tweets():\n",
    "    print(\"Loading tweets...\")\n",
    "    df = pd.read_csv('./data/2-cleaned-tweets/cleaned_tweets.csv',  encoding='utf-8', index_col=0)\n",
    "    print(\"Done.\")\n",
    "    return df  \n",
    "\n",
    "def save_dataset(df):\n",
    "    print(\"Saving dataset...\")\n",
    "    df.to_csv('./data/2-cleaned-tweets/twitter_data_clean_ham.csv')\n",
    "    print(\"Done. \\nSaved as : twitter_data_clean_ham.csv \")\n",
    "\n",
    "def clean_tweets(df):\n",
    "    # We eliminate nan fields\n",
    "    df = df[pd.notnull(df['Processed Text'])]\n",
    "\n",
    "    # We eliminate doubles\n",
    "    df = df.drop_duplicates('Message ID')\n",
    "    df = df.drop_duplicates('Processed Text')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main \n",
    "we run classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done.\n",
      "Loading tweets...\n",
      "Done.\n",
      "Pedict dataset...\n",
      "Done.\n",
      "--------------------------------------------------\n",
      "Spam number :90\t|\tHam number :101\n",
      "--------------------------------------------------\n",
      "Generating wordcloud...\n",
      "Done. \n",
      "Saved as wordcloud.png\n",
      "Saving dataset...\n",
      "Done. \n",
      "Saved as : twitter_data_clean_ham.csv \n"
     ]
    }
   ],
   "source": [
    "# main program\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # train_classifier()\n",
    "\n",
    "    # We load the model\n",
    "    model_cl = load_model()\n",
    "\n",
    "    # We load the tweets\n",
    "    df_tweets = load_tweets()\n",
    "\n",
    "    # We gather results.\n",
    "    results = predict_dataset(df_tweets, model_cl)\n",
    "\n",
    "    results = clean_tweets(results)\n",
    "\n",
    "    # Spam Or Ham\n",
    "    spam = results['Predicted']==\"spam\"\n",
    "    ham = results['Predicted']==\"ham\"\n",
    "\n",
    "    # We print out the results.\n",
    "    print(\"-\"*50)\n",
    "    print('Spam number :{0}\\t|\\tHam number :{1}'.format(len(results[spam]['Predicted']),len(results[ham]['Predicted'])))  \n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # We generate a wordcloud of the most recurrent words.\n",
    "    print(\"Generating wordcloud...\")\n",
    "    create_cloud(results[ham]['Processed Text'].to_csv(encoding='utf-8', sep=' ', index=False, header=False))\n",
    "    print(\"Done. \\nSaved as {0}\".format('wordcloud.png'))\n",
    "    \n",
    "    # We save our dataset as twitter_data_clean_ham.csv\n",
    "    save_dataset(results[ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Processed Text</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SXP</td>\n",
       "      <td>YAM</td>\n",
       "      <td>(å®Œå…¨ã«æãéª¨ã®é‡ã‚’æ¸›ã‚‰ã—ãŸã„æãæ‰‹ã®éƒ½åˆã‚‚å¤šåˆ†ã«å«ã¾ã‚Œã¾ã™)</td>\n",
       "      <td>1169866657629532160</td>\n",
       "      <td>2019-09-06 06:55:20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(å®Œå…¨ã«æãéª¨ã®é‡ã‚’æ¸›ã‚‰ã—ãŸã„æãæ‰‹ã®éƒ½åˆã‚‚å¤šåˆ†ã«å«ã¾ã‚Œã¾ã™)</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EDO</td>\n",
       "      <td>æ°·çµã“ã‚“ã¶</td>\n",
       "      <td>@t_ogiri2 ã€Œå—è›®ã‹ï¼ã€ã®ãƒã‚¿ãŒæ”¾é€ã‚³ãƒ¼ãƒ‰ã«ã‹ã‹ã£ãŸ\\nã€å¤§å–œåˆ©ã€‘æ±Ÿæˆ¸æ™‚ä»£ã‹ã‚‰ã‚¿ã‚¤ãƒ ...</td>\n",
       "      <td>1169897578239905792</td>\n",
       "      <td>2019-09-06 08:58:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ã€Œå—è›®ã‹ï¼ã€ã®ãƒã‚¿ãŒæ”¾é€ã‚³ãƒ¼ãƒ‰ã«ã‹ã‹ã£ãŸ ã€å¤§å–œåˆ©ã€‘æ±Ÿæˆ¸æ™‚ä»£ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ãƒªãƒƒãƒ—ã—ãŸãŠä¾æ¼«...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EDO</td>\n",
       "      <td>KoroGMoraza</td>\n",
       "      <td>RT @BakartxoR: Hondarribian, @Jaizkibel6-aren ...</td>\n",
       "      <td>1169897453534896132</td>\n",
       "      <td>2019-09-06 08:57:43</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Hondarribian,   desfile entsaioan, konpaini...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CRPT</td>\n",
       "      <td>Coin Trading Analytics</td>\n",
       "      <td>Top 100 avg 1h return: 0.1Â±1.0%; 55 up, 45 dow...</td>\n",
       "      <td>1169867886887702529</td>\n",
       "      <td>2019-09-06 07:00:13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Top 100 avg 1h return: 0.1Â±1.0%; 55 up, 45 dow...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SXP</td>\n",
       "      <td>ã¬ã—ãˆ</td>\n",
       "      <td>@yy109810 ã‚²ãƒ¬ãƒ³ãƒ‡ãŒæº¶ã‘ã‚‹ã»ã©æ‹ã—ãŸã„</td>\n",
       "      <td>1169887761102360576</td>\n",
       "      <td>2019-09-06 08:19:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ã‚²ãƒ¬ãƒ³ãƒ‡ãŒæº¶ã‘ã‚‹ã»ã©æ‹ã—ãŸã„</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Company Name             Author Name  \\\n",
       "10          SXP                     YAM   \n",
       "7           EDO                   æ°·çµã“ã‚“ã¶   \n",
       "8           EDO             KoroGMoraza   \n",
       "8          CRPT  Coin Trading Analytics   \n",
       "0           SXP                     ã¬ã—ãˆ   \n",
       "\n",
       "                                                 Text           Message ID  \\\n",
       "10                    (å®Œå…¨ã«æãéª¨ã®é‡ã‚’æ¸›ã‚‰ã—ãŸã„æãæ‰‹ã®éƒ½åˆã‚‚å¤šåˆ†ã«å«ã¾ã‚Œã¾ã™)  1169866657629532160   \n",
       "7   @t_ogiri2 ã€Œå—è›®ã‹ï¼ã€ã®ãƒã‚¿ãŒæ”¾é€ã‚³ãƒ¼ãƒ‰ã«ã‹ã‹ã£ãŸ\\nã€å¤§å–œåˆ©ã€‘æ±Ÿæˆ¸æ™‚ä»£ã‹ã‚‰ã‚¿ã‚¤ãƒ ...  1169897578239905792   \n",
       "8   RT @BakartxoR: Hondarribian, @Jaizkibel6-aren ...  1169897453534896132   \n",
       "8   Top 100 avg 1h return: 0.1Â±1.0%; 55 up, 45 dow...  1169867886887702529   \n",
       "0                            @yy109810 ã‚²ãƒ¬ãƒ³ãƒ‡ãŒæº¶ã‘ã‚‹ã»ã©æ‹ã—ãŸã„  1169887761102360576   \n",
       "\n",
       "           Published At  Retweet Count  Favorite Count  \\\n",
       "10  2019-09-06 06:55:20              0               0   \n",
       "7   2019-09-06 08:58:12              0               0   \n",
       "8   2019-09-06 08:57:43             19               0   \n",
       "8   2019-09-06 07:00:13              0               0   \n",
       "0   2019-09-06 08:19:12              0               0   \n",
       "\n",
       "                                       Processed Text Predicted  \n",
       "10                    (å®Œå…¨ã«æãéª¨ã®é‡ã‚’æ¸›ã‚‰ã—ãŸã„æãæ‰‹ã®éƒ½åˆã‚‚å¤šåˆ†ã«å«ã¾ã‚Œã¾ã™)       ham  \n",
       "7     ã€Œå—è›®ã‹ï¼ã€ã®ãƒã‚¿ãŒæ”¾é€ã‚³ãƒ¼ãƒ‰ã«ã‹ã‹ã£ãŸ ã€å¤§å–œåˆ©ã€‘æ±Ÿæˆ¸æ™‚ä»£ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ãƒªãƒƒãƒ—ã—ãŸãŠä¾æ¼«...       ham  \n",
       "8      Hondarribian,   desfile entsaioan, konpaini...       ham  \n",
       "8   Top 100 avg 1h return: 0.1Â±1.0%; 55 up, 45 dow...       ham  \n",
       "0                                      ã‚²ãƒ¬ãƒ³ãƒ‡ãŒæº¶ã‘ã‚‹ã»ã©æ‹ã—ãŸã„       ham  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[ham].sample(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Text</th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Published At</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Processed Text</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRPT</td>\n",
       "      <td>Energiwik</td>\n",
       "      <td>@Louna_Crpt @JulieClerissy Si si justement fai...</td>\n",
       "      <td>1169886961508610049</td>\n",
       "      <td>2019-09-06 08:16:01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Si si justement fais attention Ã  toi ğŸ˜ˆ</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DROP</td>\n",
       "      <td>CryptoAdvocate ğŸ‡¦ğŸ‡º</td>\n",
       "      <td>Always great to see a new #MCO #VISA Card reci...</td>\n",
       "      <td>1169885979613614080</td>\n",
       "      <td>2019-09-06 08:12:07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Always great to see a new     Card recipient p...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEXO</td>\n",
       "      <td>á±¬; êªá¥²êª€dá¥² â„³ (Menciones|Disney! Au)à­­Ì¥à³„</td>\n",
       "      <td>@Rusbrk á±¬âµ“ â€œ No,Â no me molesta, depende de com...</td>\n",
       "      <td>1169899045034180610</td>\n",
       "      <td>2019-09-06 09:04:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>á±¬âµ“ â€œ No,Â no me molesta, depende de como lo d...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NEXO</td>\n",
       "      <td>MegaNerd</td>\n",
       "      <td>@dynitstaff e @Nexo_Digital continuano a porta...</td>\n",
       "      <td>1169895570380476416</td>\n",
       "      <td>2019-09-06 08:50:14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>e   continuano a portare gli   nei cinema it...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EDO</td>\n",
       "      <td>Om edo</td>\n",
       "      <td>@BuuloloArianto Berkali-kali studi banding ke ...</td>\n",
       "      <td>1169898699901718531</td>\n",
       "      <td>2019-09-06 09:02:40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Berkali-kali studi banding ke luar negeri hs...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company Name                           Author Name  \\\n",
       "0         CRPT                             Energiwik   \n",
       "1         DROP                     CryptoAdvocate ğŸ‡¦ğŸ‡º   \n",
       "0         NEXO  á±¬; êªá¥²êª€dá¥² â„³ (Menciones|Disney! Au)à­­Ì¥à³„   \n",
       "6         NEXO                              MegaNerd   \n",
       "8          EDO                                Om edo   \n",
       "\n",
       "                                                Text           Message ID  \\\n",
       "0  @Louna_Crpt @JulieClerissy Si si justement fai...  1169886961508610049   \n",
       "1  Always great to see a new #MCO #VISA Card reci...  1169885979613614080   \n",
       "0  @Rusbrk á±¬âµ“ â€œ No,Â no me molesta, depende de com...  1169899045034180610   \n",
       "6  @dynitstaff e @Nexo_Digital continuano a porta...  1169895570380476416   \n",
       "8  @BuuloloArianto Berkali-kali studi banding ke ...  1169898699901718531   \n",
       "\n",
       "          Published At  Retweet Count  Favorite Count  \\\n",
       "0  2019-09-06 08:16:01              0               0   \n",
       "1  2019-09-06 08:12:07              1               1   \n",
       "0  2019-09-06 09:04:02              0               0   \n",
       "6  2019-09-06 08:50:14              0               0   \n",
       "8  2019-09-06 09:02:40              0               0   \n",
       "\n",
       "                                      Processed Text Predicted  \n",
       "0             Si si justement fais attention Ã  toi ğŸ˜ˆ      spam  \n",
       "1  Always great to see a new     Card recipient p...      spam  \n",
       "0    á±¬âµ“ â€œ No,Â no me molesta, depende de como lo d...      spam  \n",
       "6    e   continuano a portare gli   nei cinema it...      spam  \n",
       "8    Berkali-kali studi banding ke luar negeri hs...      spam  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[spam].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Conclusion\n",
    "To conclude, we sadly, can see that our algorithm isn't perfect. We realized the importance of good data. Machine learning algorithms only performs well when there is high quality in the data. Furthermore, we looked for freely available data and did not take any costly solution into account. To improve the accuracy of our algorithm one solution would be to better understand how a spam is structured and find characteristics that differentiate it to a ham. That could be done by adding additional requirements that a tweet should pass in order to be classified as ham. Such as for example **weird characters**, **smileys**, **SMS words**, **better filter for languages**, **etc...**\n",
    "\n",
    "Our main concern would reside in the fact that we are more **interested** in elminating the **false negative** than we are in eliminating **false positive**. As example, that would matter more that a **spam is classified as ham** than the inverse. Our **false negative** ham would lead to a **bias** in our sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the user may go to Part - 4 Sentiment Analysis\n",
    "File *sentiment_analysis.ipynb* in folder **4-Sentiment**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".data-science",
   "language": "python",
   "name": ".data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
